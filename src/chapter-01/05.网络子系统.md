## 网络子系统
网络子系统是性能方面的另一个重要子系统。网络操作与许多 Linux 以外的组件交互，如交换机、路由器、网关、PC 客户机等等。尽管这些组件可能不在 Linux 的控制范围内，但它们对整体性能有很大的影响。请记住，您必须与从事网络系统工作的人员密切合作。

这里我们主要关注 Linux 如何处理网络操作。

### 网络实现
TCP/IP 协议具有类似于 OSI 层模型的分层结构。Linux 内核网络实现采用了类似的方法。[@fig:network_overview] 展示了 Linux TCP/IP 协议栈的层次，并提供了 TCP/IP 通信的概述。

![网络分层结构和组网操作概述](images/network_overview.jpg){#fig:network_overview}

与许多 UNIX 系统一样，Linux 使用套接字（_socket_）接口进行 TCP/IP 网络操作。套接字为用户应用程序提供了一个接口。我们将看一看在网络数据传输过程中发生的基本操作的序列。

1. 当应用程序向它的对等主机发送数据时，应用程序创建它的数据。
2. 应用程序打开套接字并通过套接字接口写入数据。
3. 套接字缓冲区（_socket buffer_）用于处理传输的数据。套接字缓冲区有对数据的引用，它向下穿过各个层。
4. 在每一层中，都会执行适当的操作，如解析数据包头、添加和修改数据包头、校验包头（_check sums_）、路由操作、分段等。当套接字缓冲区向下穿过各层时，数据本身不会在各层之间复制。由于在不同层之间复制实际数据效率低下，内核通过更改套接字缓冲区中的引用并将其传递给下一层来避免不必要的开销。
5. 最后，数据从网卡输出到网线。
6. 以太网帧到达对端主机的网络接口。
7. 如果目的 MAC 地址匹配对端网卡的 MAC 地址，帧被移动到网卡缓冲区。
8. 网卡最终将包移动到套接字缓冲区，并向 CPU 发出一个硬中断。
9. 然后，中央处理器处理数据包，并将其向上移动，直到它到达应用程序（如 Apache）的 TCP 端口。

#### 套接字缓冲区（Socket buffer）
如前所述，内核使用缓冲区来发送和接收数据。[@fig:socket_buffer] 显示了网络中可配置的缓冲区。它们可以通过 `/proc/sys/net` 下的文件进行调优。

```
/proc/sys/net/core/rmem_max
/proc/sys/net/core/rmem_default
/proc/sys/net/core/wmem_max
/proc/sys/net/core/wmem_default
/proc/sys/net/ipv4/tcp_mem
/proc/sys/net/ipv4/tcp_rmem
/proc/sys/net/ipv4/tcp_wmem
```
有时它可能对网络性能有影响。我们将在 4.7.4 节的“增加网络缓冲区”中讨论细节。

![套接字缓冲区内存分配](images/socket_buffer.jpg){#fig:socket_buffer width=60%}

#### Network API (NAPI)
随着新的网络 API (NAPI) 的引入，网络子系统发生了一些变化。Linux 中网络堆栈的标准实现更多地关注于可靠性和低延迟，而不是低开销和高吞吐量。虽然这些特性在创建防火墙时是有利的，但大多数企业应用程序（如文件和打印或数据库）的执行速度会比 Windows® 下的类似程序慢。

在处理网络数据包的传统方法中，如 [@fig:network_stack] 中蓝色箭头所示，网络接口卡最终将数据包移动到操作系统内核的网络缓冲区，并向 CPU 发出硬中断。

这只是处理网络数据包过程的一个简化视图，但它说明了这种方法的缺点之一。每次有一个 MAC 地址匹配的以太网帧到达接口时，就会有一个硬中断。每当 CPU 必须处理硬中断时，它必须停止正在处理的任何事情，并处理中断，从而导致上下文切换和相关的处理器缓存刷新。虽然您可能认为，如果只有几个包到达接口，那么这不是问题，但千兆以太网和现代应用程序每秒可以创建数千个包，从而导致发生大量中断和上下文切换。

![Linux 网络栈](images/network_stack.jpg){#fig:network_stack width=60%}

因此，引入了 NAPI 来抵消与处理网络流量相关的开销。对于第一个包，NAPI 的工作原理与传统实现一样，它为第一个包发出中断。但是在第一个包之后，接口进入轮询（polling）模式。只要网络接口的 DMA 环缓冲区（_ring buffer_）中有数据包，就不会产生新的中断，有效地减少了上下文切换和相关的开销。如果最后一个包被处理，并且环形缓冲区被清空，那么接口卡将再次退回到中断模式。NAPI 还通过创建可由多个处理器处理的软中断，提高了多处理器的可伸缩性。虽然 NAPI 对于大多数企业级多处理器系统来说是一个巨大的改进，但它需要支持 NAPI 的驱动程序。这里有很大的调优空间，我们将在本文的调优部分进行探讨。

#### Netfilter
Linux 具有作为内核一部分的高级防火墙功能。此功能由 _Netfilter_ 模块提供。您可以使用 **iptables** 实用程序操作和配置 Netfilter。

一般来说，Netfilter 提供以下功能。

- 包过滤：当报文命中某条规则时，Netfilter 将接受或拒绝该报文，或者根据定义的规则采取相应的处理措施。
- 地址转换（Address translation）：如果报文命中了某个规则，Netfilter 将对报文进行修改，以满足地址转换的要求。

匹配过滤可以定义以下属性。

- 网络接口（Network Interface）
- IP 地址，IP 地址范围，子网
- 协议
- ICMP 类型
- 端口
- TCP 标志
- 状态（参见下文”连接追踪“）

[@fig:netfilter] 给出了数据包如何通过 Netfilter 链的概述，Netfilter 链是应用在每个点上的定义规则的列表。

![Netfilter 中数据包的流向](images/netfilter.jpg){#fig:netfilter}

如果数据包符合规则，Netfilter 将采取适当的行动。这个动作称为 target。一些可能的 target 有：

ACCEPT
:    接受数据包，让其通过。

DROP
:    静默的将数据包丢弃。

REJECT
:    丢弃数据包，并且向发送端回传数据包如 ICMP 端口不可达。TCP 重置为原始主机。

LOG
:    记录匹配的数据包。

MASQUERADE, SNAT, DNAT, REDIRECT
:    地址转换。

#### 连接追踪（Connection tracking）
为了实现更复杂的防火墙功能，Netfilter 使用连接跟踪机制来跟踪所有网络流量的状态。Netfilter 根据 TCP 连接状态（参见下节的“建立连接”) 和其他网络属性（如 IP 地址、端口、协议、序列号（sequence number）、确认号（ack number）、ICMP 类型等），将每个数据包分为以下四种状态。

NEW
:    试图建立新连接的包

ESTABLISHED
:    数据包通过已建立的连接

RELATED
:    与以前数据包相关的数据包

INVALID
:    由于损坏或无效而处于未知状态的数据包

此外，Netfilter 可以使用一个单独的模块，通过分析协议特定的属性和操作来执行更详细的连接跟踪。例如，有用于 FTP、NetBIOS、TFTP、IRC 等的连接跟踪模块。

### TCP/IP
TCP/IP 多年来一直是默认的网络协议。Linux TCP/IP 实现与它的标准相当兼容。为了获得更好的性能调优，您应该熟悉基本的 TCP/IP 网络。

更多细节，请参阅 _TCP/IP Tutorial and Technical Overview_，GG24-3376。

#### 连接的建立
在传输应用程序数据之前，应该在客户端和服务端之间建立连接。建立连接的过程称为 TCP/IP 三次握手。[@fig:tcp_hand_shake] 描述了基本的连接建立和终止过程。

1. 客户端向对端服务器发送 SYN 包（设置了 SYN 标志的包）请求建立连接。
2. 服务端收到之后响应 SYN+ACK 包
3. 然后客户端向对端发送 ACK 报文，完成连接的建立。

一旦建立了连接，就可以通过连接传输应用程序数据。当所有数据传输完毕后，连接关闭过程开始。

1. 客户端向服务器发送 FIN 包，启动连接终止过程。
2. 服务器返回 FIN 的确认信息，如果没有数据可以发送给客户端，则将 FIN 报文发送给客户端。
3. 客户端向服务端发送 ACK 报文，结束连接。

![TCP 三次握手和四次挥手](images/tcp_hand_shake.jpg){#fig:tcp_hand_shake width=60%}

连接的状态在会话期间改变。TCP/IP 连接状态图如 [@fig:tcp_state] 所示。

![TCP 连接状态图](images/tcp_state.jpg){#fig:tcp_state width=80%}

使用 **netstat** 命令可以查看每个 TCP/IP 会话的连接状态。详情请参见 2.3.11 节的 “netstat”。

#### 流量控制（Traffic control）
TCP/IP 实现具有保证数据高效传输的机制，即使在网络传输质量差、拥塞的情况下也能保证数据包的传输。

#### TCP/IP 传输窗口（transfer window）
在 Linux 操作系统中，传输窗口的原理是 TCP/IP 实现中性能方面的一个重要方面。基本上，TCP 传输窗口是给定主机在要求连接另一端的确认之前可以发送或接收的最大数据量。窗口大小由接收主机提供给发送主机，由 TCP 头中的窗口大小字段提供。使用传输窗口，主机可以更有效地发送数据包，因为发送主机不必等待对每个发送数据包的确认。它使网络得到更多的利用。延迟确认也提高了效率。TCP 窗口开始时很小，随着连接另一端的每次成功确认，窗口慢慢增加。要优化窗口大小，请参阅 4.7.4 节 “增加网络缓冲区”

![滑动窗口和延迟确认（delayed ack）](images/sliding_window.jpg){#fig:sliding_window}

作为一种选择，高速网络可以使用一种称为窗口缩放（_window scaling_）的技术来进一步增加最大传输窗口大小。我们将在 4.7.5 节的 “调优 TCP 选项” 中详细分析这些实现的效果。

#### 重传（Retransmission）
在连接建立和终止以及数据传输过程中，由于各种原因（网络接口故障、路由器速度慢、网络拥塞、网络实现错误等）会导致很多超时和数据重传。TCP/IP 通过对数据包排队并多次尝试发送数据包来处理这种情况。

您可以通过配置参数来改变内核的某些行为。在丢包率高的网络上，您可能希望增加 TCP SYN 连接建立数据包的尝试次数。您还可以通过`/proc/sys/net` 下的文件更改一些超时阈值。有关更多信息，请参阅 4.7.5 节的 “TCP 行为调优”。

### 卸载（Offload）
如果系统上的网络适配器支持硬件卸载功能，内核就可以将部分任务卸载给适配器，这样就可以减少 CPU 利用率。

校验和卸载（Checksum offload）
:    IP/TCP/UDP 校验和是通过比较协议头中校验和字段的值和数据包数据的计算值来确保数据包正确传输。

TCP segmentation offload（TSO）
:    当发送到网卡的数据大于 MTU (maximum transmission unit) 时，需要将数据分成 MTU 大小的数据包。适配器代表内核处理这些问题。

有关更多高级网络特性的信息，请参阅 _Tuning IBM System x Servers for
Performance_，SG24-5287。10.3 节。先进的网络特性。

### Bonding 模块
Linux 内核通过使用绑定（_bonding_）驱动程序提供网络接口聚合功能。这是一个独立于设备的绑定驱动程序（也有特定于设备的驱动程序）。bonding 驱动支持 802.3 链路聚合规范和一些原始的负载均衡和容错实现。它实现了更高级别的可用性和性能改进。请参考内核文档 **Documentation/networking/bonding.txt**。