## 磁盘 I/O 子系统
在处理器解码和执行指令之前，先从磁盘扇区中读取数据并存入处理器缓存和寄存器中。执行的结果会被写回磁盘。

我们将研究 Linux 磁盘 I/O 子系统，以便更好地理解对系统性能有重大影响的组件。

### I/O 子系统架构
I/O 子系统的基本概念如 [@fig:io_arch] 所示。

![I/O 子系统架构](images/io_arch.jpg){#fig:io_arch width=70%}

为了快速概述整个 I/O 子系统操作，我们将使用一个将数据写入磁盘的示例。下面的步骤概述了执行磁盘写操作时发生的基本操作。假设文件数据在磁盘上的扇区上，已经被读取，并且在页面缓存中。

1. 进程通过 `write()` 系统调用请求写入文件。
2. 内核更新映射到文件的页面缓存（_page cache_）。
3. `pdflush` 内核线程负责将页面缓存刷新到磁盘。
4. 文件系统层将每个块缓冲区放在一个 _bio_ 结构中（参见 1.4.3 节中的 “Block layer” )，并向块设备层提交写请求。
5. 块设备层从上层获取请求，执行 I/O 调度（_I/O elevator_）操作，并将请求放入 I/O 请求队列中。
6. 设备驱动程序（如 SCSI 或其他特定于设备的驱动程序）将负责写操作。
7. 磁盘设备固件执行硬件操作，如查找磁头、旋转和数据传输到盘片上的扇区。

### 缓存
在过去的 20 年里，处理器的性能改进已经超过了计算机系统中的其他组件，如处理器缓存、总线、RAM、磁盘等。对内存和磁盘的较慢访问限制了整体系统性能，因此系统性能不能通过处理器速度的提高而得到提高。缓存机制通过在更快的内存中缓存频繁使用的数据来解决这个问题。它减少了访问较慢内存的机会。目前的计算机系统几乎在所有 I/O 组件中都使用了这种技术，如硬盘驱动器缓存、磁盘控制器缓存、文件系统缓存、由每个应用程序处理的缓存，等等。

#### 内存层级（Memory Hierarchy）
[@fig:memory_hierarchy] 展示了内存层次结构的概念。由于 CPU 寄存器和磁盘之间的访问速度差异很大，CPU 将花费更多的时间等待来自低速磁盘设备的数据，因此这将大大降低 CPU 的优势。内存层次结构通过在 CPU 和磁盘之间放置 L1 缓存、L2 缓存、RAM 和其他一些缓存来减少这种不匹配。它使进程获得更少的机会访问较慢的内存和磁盘。靠近处理器的内存速度更快，体积更小。

该技术还利用了访问局部性原则（_locality of reference principle_）。在更快的内存中，缓存命中率越高，对数据的访问就越快。

![内存层级](images/memory_hierarchy.jpg){#fig:memory_hierarchy}

#### 访问局部性（Locality of reference）
正如我们在前面的“内存层级”中所述，实现更高的缓存命中率是提高性能的关键。为了达到更高的缓存命中率，使用了一种叫做“访问局部性”的技术。该技术基于以下原则：

- 最近使用的数据很有可能在不久的将来被使用（时间局部性 _temporal locality_）。
- 与已使用的数据相近的数据被使用的概率很高（空间局部性 _spatial locality_）。

[@fig:locality] 说明了此原则。

![访问局部性](images/locality.jpg){#fig:locality width=80%}

Linux 在许多组件中使用了这一原则，例如页面缓存、文件对象缓存 (i-node 缓存、目录条目缓存，等等）、预读缓存（_read ahead buffer_）等等。

#### 刷新脏缓冲（Flushing a dirty buffer）
当进程从磁盘读取数据时，数据被复制到内存中。进程和其他进程可以从缓存在内存中的数据副本中检索相同的数据。当进程试图更改数据时，它首先更改内存中的数据。此时，磁盘上的数据和内存中的数据并不相同，内存中的数据被称为脏缓冲区（_dirty buffer_）。脏缓冲区应该尽快与磁盘上的数据同步，否则如果突然发生崩溃，内存中的数据可能会丢失。

脏缓冲区的同步过程称为刷新（_flush_）。在 Linux 内核 2.6 实现中，`pdflush` 内核线程负责将数据刷新到磁盘。当内存中脏缓冲区的比例超过某个阈值 (bdflush) 时，刷新会定期发生 (kupdate)。阈值可在 `/proc/sys/vm/dirty_background_ratio` 文件里调整。有关更多信息，请参阅 4.5.1 节，“设置内核交换和 pdflush 行为”。

![刷新脏缓冲](images/flushing_dirty_buffers.jpg){#fig:flushing_dirty_buffers width=80%}

### 块设备层
块层处理所有与块设备操作相关的活动（参见 [@fig:io_arch])。块层的关键数据结构是 _bio_ 结构。_bio_ 结构是文件系统层和块层之间的接口。

当执行写操作时，文件系统层尝试写入由块缓冲区组成的页缓存。它通过将连续的块放在一起组成一个 _bio_ 结构，然后将 _bio_ 发送到块层。（参考 [@fig:io_arch])

块层处理 _bio_ 请求并将这些请求链接到一个称为 I/O 请求队列的队列中。这种连接操作被称为 I/O 调度器（_I/O elevator_）。在 Linux 内核 2.6 实现中，有四种类型的 I/O elevator 算法可用。它们是：

#### Block sizes
块大小（block size），即可以读或写到驱动器的最小数据量，可以直接影响服务器的性能。作为指导原则，如果您的服务器正在处理大量小文件，那么较小的块大小将更有效。如果您的服务器专用于处理大文件，那么更大的块大小可能会提高性能。不能在现有文件系统上动态更改块大小。只有重新格式化才会修改当前块的大小。

#### I/O elevator
Linux 内核 2.6 采用了一个新的 I/O 调度模型。Linux 内核 2.4 使用了一个通用的 I/O 调度器，而内核 2.6 提供了四个调度算法的选择。因为 Linux 操作系统可以用于广泛的任务，所以 I/O 设备和工作负载特征都发生了很大的变化。笔记本电脑可能与 10,000 个用户的数据库系统有不同的 I/O 要求。为了适应这种情况，有四个 I/O 调度算法可供使用。

Anticipatory
:    Anticipatory 算法是基于块设备只有一个物理寻道头（例如单个 SATA 驱动器）的假设创建的。Anticipatory 算法使用了截止日期机制加上一个预测启发式（下面会详细描述）。顾名思义，Anticipatory 算法“预测”I/O，并尝试将其以单个、较大的流写入磁盘，而不是多个非常小的随机磁盘访问。预期启发式可能会导致写 I/O 延迟。它显然是为一般的个人计算机等通用系统的高吞吐量进行了优化。一直到内核版本 2.6.18，Anticipatory 算法都是标准 I / O 调度器算法。然而，大多数 Enterprise Linux 发行版默认使用 CFQ 算法。

完全公平队列（Complete Fair Queuing  CFQ）
:    CFQ 调度算法通过维护每个进程的 I/O 队列来实现进程的 QoS（服务质量）策略。CFQ 调度器非常适合具有许多竞争进程的大型多用户系统。它积极地尝试避免进程饿死，并具有低延迟的特点。从内核版本 2.6.18 开始，改进的 CFQ 调度器是默认的 I/O 调度器。

    根据系统设置和工作负载特征，CFQ 调度器可以降低单个主应用程序的速度，例如一个具有面向公平性算法的大型数据库。默认配置基于进程组之间的竞争来处理公平性。例如，CFQ 将单个数据库和所有通过页面缓存的写操作（所有 pdflush 实例都在一个 pgroup 中）视为单个应用程序，可以与许多后台进程竞争。在这种情况下，使用 I/O 调度器子配置和/或 Deadline 调度器进行试验是很有用的。

Deadline
:    Deadline 调度算法是一个循环调度器（轮询 _round robin_），它有一个提供 I/O 子系统近乎实时行为的最后期限算法。Deadline 调度器提供了极佳的请求延迟，同时保持了良好的磁盘吞吐量。最后期限算法的实现确保了进程不会出现饥饿现象。

NOOP
:    NOOP 代表 No Operation，这个名称解释了它的大部分功能。NOOP 调度器简单直接。它是一个简单的 FIFO 队列，不执行任何数据排序。NOOP 只是合并相邻的数据请求，因此它为磁盘 I/O 增加了非常低的处理器开销。NOOP 调度器假设块设备具有自己的调度器算法（如用于 SCSI 的 TCQ)，或者没有查找延迟的块设备如闪存卡。

::: {.note}
在 Linux 2.6.18 内核中可以为每个硬盘子系统设定特定的 I/O 调度器，不需要在系统级上设置。
:::

### I/O 设备驱动
Linux 内核使用设备驱动程序控制设备。设备驱动程序通常是一个独立的内核模块，并为每个设备（或设备组）提供该驱动程序，以使该设备可用于 Linux 操作系统。一旦设备驱动程序被加载，它就作为 Linux 内核的一部分运行，并完全控制设备。这里我们描述 SCSI 设备驱动程序。

#### SCSI
小型计算机系统接口 (Small Computer System Interface, SCSI) 是最常用的 I/O 设备技术，特别是在企业服务器环境中。在 Linux 内核实现中，SCSI 设备由设备驱动程序模块控制。它们由以下类型的模块组成。

- 上层驱动：sd_mod、sr_mod (SCSI- cdrom)、st (SCSI 磁带）、sq (SCSI 通用设备）等等。提供支持几种 SCSI 设备的功能，如 SCSI CD-ROM，SCSI 磁带，等等。
- 中层驱动程序：scsi_mod。实现 SCSI 协议和通用 SCSI 功能。
- 底层驱动程序。提供对每个设备的低级访问。底层驱动程序基本上是特定于硬件设备的，并为每个设备提供。例如，ips 用于 IBM ServeRAID™控制器，qla2300 用于 Qlogic HBA, mptscsih 用于 LSI Logic SCSI 控制器，等等。
- 伪驱动：ide-scsi。用于 IDE-SCSI 模拟。

![SCSI 驱动的结构](images/scsi_drivers.jpg){#fig:scsi_drivers}

如果特定的功能是为一个设备实现的，它应该在设备固件和低级设备驱动程序中实现。所支持的功能取决于您使用的硬件和您使用的设备驱动程序的版本。设备本身也应该支持所需的功能。特定函数通常由设备驱动程序参数进行调优。您可以在/etc/modules.conf 中尝试一些性能优化。有关调优提示和技巧，请参阅设备和设备驱动程序文档。

### RAID 和存储系统
存储系统和 RAID 类型的选择和配置也是影响系统性能的重要因素。Linux 支持软件 RAID，但本主题的详细内容不在本文讨论范围之内。我们在 4.6.1 “安装 Linux 之前的硬件考虑” 中包含了一些调优注意事项。

有关可用 IBM 存储解决方案的详细信息，请参见：
- Tuning IBM System x Servers for Performance, SG24-5287
- IBM System Storage Solutions Handbook, SG24-5250
- Introduction to Storage Area Networks, SG24-5470